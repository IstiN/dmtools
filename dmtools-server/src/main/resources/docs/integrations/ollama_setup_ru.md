# Руководство по настройке интеграции Ollama

## Обзор
Это руководство поможет вам настроить интеграцию Ollama для локальных ИИ-возможностей. Ollama позволяет запускать большие языковые модели локально на вашей машине без необходимости использования облачных API-ключей.

## Предварительные требования
- Ollama установлен на вашей локальной машине или сервере
- Загружена хотя бы одна модель Ollama
- Достаточные системные ресурсы (RAM и CPU/GPU) для запуска модели

## Шаг 1: Установка Ollama

### Для macOS и Linux

1. **Установите Ollama**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Проверьте установку**
   ```bash
   ollama --version
   ```

### Для Windows

1. **Скачайте Ollama**
   - Посетите [ollama.com](https://ollama.com/download)
   - Скачайте установщик для Windows
   - Запустите установщик и следуйте инструкциям

2. **Проверьте установку**
   - Откройте Командную строку или PowerShell
   - Выполните: `ollama --version`

## Шаг 2: Загрузка модели

Перед использованием Ollama необходимо загрузить хотя бы одну модель:

### Популярные модели

1. **Llama 3 (Рекомендуется)**
   ```bash
   ollama pull llama3
   ```
   - Хороший баланс производительности и качества
   - Требует ~4GB RAM

2. **Mistral**
   ```bash
   ollama pull mistral
   ```
   - Быстрая и эффективная
   - Требует ~4GB RAM

3. **CodeLlama (Для задач с кодом)**
   ```bash
   ollama pull codellama
   ```
   - Оптимизирована для генерации кода
   - Требует ~4GB RAM

4. **Llama 3.1 (Последняя версия)**
   ```bash
   ollama pull llama3.1
   ```
   - Последняя версия с улучшениями
   - Требует ~4GB RAM

### Просмотр доступных моделей
```bash
ollama list
```

## Шаг 3: Запуск сервиса Ollama

Ollama работает как фоновый сервис:

### macOS и Linux
```bash
ollama serve
```

Сервис обычно запускается автоматически после установки и работает на `http://localhost:11434`

### Windows
- Сервис Ollama запускается автоматически после установки
- Проверьте, работает ли он, открыв `http://localhost:11434` в браузере

### Проверка работы сервиса
```bash
curl http://localhost:11434/api/tags
```

Вы должны увидеть JSON-ответ с доступными моделями.

## Шаг 4: Настройка интеграции в DMTools

При настройке интеграции Ollama в DMTools используйте следующие значения:

### Обязательные параметры

1. **Модель** (Обязательно)
   - Название модели Ollama для использования
   - Примеры:
     - `llama3`
     - `mistral`
     - `codellama`
     - `llama3.1`
   - **Важно**: Модель должна быть предварительно загружена командой `ollama pull <модель>`

### Необязательные параметры

2. **Базовый путь** (Необязательно)
   - По умолчанию: `http://localhost:11434`
   - Изменяйте только если:
     - Ollama работает на другом порту
     - Ollama работает на удаленном сервере
   - Примеры:
     - Локальный пользовательский порт: `http://localhost:8080`
     - Удаленный сервер: `http://192.168.1.100:11434`

3. **Размер контекстного окна** (Необязательно)
   - По умолчанию: `16384` токенов
   - Контролирует, сколько контекста модель может запомнить
   - Большие значения:
     - Позволяют больше контекста в диалогах
     - Требуют больше памяти
     - Могут замедлить ответы
   - Рекомендуемые значения:
     - Малые задачи: `4096`
     - Обычные задачи: `16384`
     - Большие документы: `32768`

4. **Максимум токенов предсказания** (Необязательно)
   - По умолчанию: `-1` (неограниченно)
   - Контролирует максимальную длину ответов ИИ
   - Установите положительное число для ограничения длины ответа
   - Примеры:
     - Короткие ответы: `512`
     - Обычные ответы: `2048`
     - Длинные ответы: `4096`
     - Неограниченно: `-1`

## Шаг 5: Тестирование соединения

После настройки:
1. Используйте функцию тестирования соединения в DMTools
2. Проверьте, что вы можете связаться с Ollama
3. Убедитесь, что ответы генерируются

## Примеры конфигурации

### Базовая конфигурация (Локально)
```
Модель: llama3
Базовый путь: http://localhost:11434
Размер контекстного окна: 16384
Максимум токенов предсказания: -1
```

### Конфигурация удаленного сервера
```
Модель: mistral
Базовый путь: http://192.168.1.100:11434
Размер контекстного окна: 8192
Максимум токенов предсказания: 2048
```

### Конфигурация для генерации кода
```
Модель: codellama
Базовый путь: http://localhost:11434
Размер контекстного окна: 32768
Максимум токенов предсказания: 4096
```

## Соображения производительности

### Системные требования

**Минимальные требования:**
- RAM: 8GB
- CPU: Современный многоядерный процессор
- Хранилище: 5GB для файлов моделей

**Рекомендуемые требования:**
- RAM: 16GB или больше
- GPU: NVIDIA GPU с поддержкой CUDA (необязательно, но значительно быстрее)
- Хранилище: 10GB для нескольких моделей

### Выбор модели

- **Малые модели** (7B параметров): Быстрее, менее точные, меньше использование памяти
- **Средние модели** (13B параметров): Сбалансированная производительность и качество
- **Большие модели** (70B+ параметров): Лучшее качество, медленнее, высокое использование памяти

### Ускорение GPU

Ollama автоматически использует GPU, если доступен:
- **NVIDIA GPUs**: Автоматически обнаруживаются и используются
- **Apple Silicon (M1/M2/M3)**: Автоматически использует Metal
- **AMD GPUs**: Ограниченная поддержка, проверьте документацию Ollama

## Устранение неполадок

### Распространенные проблемы

1. **Не удалось подключиться**
   - Проверьте, что сервис Ollama запущен: `curl http://localhost:11434/api/tags`
   - Проверьте правильность URL базового пути
   - Убедитесь, что брандмауэр не блокирует соединение
   - Попробуйте перезапустить Ollama: `ollama serve`

2. **Модель не найдена**
   - Проверьте, что модель загружена: `ollama list`
   - Загрузите модель, если отсутствует: `ollama pull <имя-модели>`
   - Проверьте правильность написания имени модели (с учетом регистра)

3. **Недостаточно памяти**
   - Используйте меньшую модель (например, `mistral` вместо `llama3.1:70b`)
   - Уменьшите размер контекстного окна
   - Закройте другие приложения
   - Рассмотрите возможность увеличения RAM системы

4. **Медленные ответы**
   - Используйте меньшую модель
   - Уменьшите размер контекстного окна
   - Включите ускорение GPU, если доступно
   - Проверьте использование системных ресурсов

5. **Сервис не запускается**
   - Проверьте, не занят ли порт 11434
   - Просмотрите логи Ollama на наличие ошибок
   - Попробуйте переустановить Ollama
   - Проверьте системные требования

### Получение помощи

Если вы столкнулись с проблемами:
1. Проверьте документацию Ollama: [ollama.com/docs](https://ollama.com/docs)
2. Убедитесь, что ваша модель совместима с вашей системой
3. Проверьте проблемы на GitHub Ollama: [github.com/ollama/ollama](https://github.com/ollama/ollama)
4. Просмотрите системные логи на наличие сообщений об ошибках

## Соображения безопасности

- **Локальное выполнение**: Ollama работает локально, данные не отправляются на внешние серверы
- **Сетевой доступ**: По умолчанию Ollama принимает только локальные соединения
- **Удаленный доступ**: Если вы предоставляете Ollama удаленный доступ, используйте правильные правила брандмауэра и аутентификацию
- **Доверие к моделям**: Загружайте модели только из надежных источников

## Расширенная конфигурация

### Запуск Ollama на пользовательском порту
```bash
OLLAMA_HOST=0.0.0.0:8080 ollama serve
```

### Использование ускорения GPU
Ollama автоматически обнаруживает и использует доступные GPU. Для проверки:
```bash
ollama run llama3 "Привет" --verbose
```

### Несколько моделей
Вы можете установить несколько моделей и переключаться между ними:
```bash
ollama pull llama3
ollama pull mistral
ollama pull codellama
```

Затем настройте разные интеграции DMTools для каждой модели.

## Дополнительные ресурсы

- [Официальный сайт Ollama](https://ollama.com)
- [Документация Ollama](https://ollama.com/docs)
- [Репозиторий GitHub Ollama](https://github.com/ollama/ollama)
- [Библиотека доступных моделей](https://ollama.com/library)
- [Документация API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)

## Рекомендации по моделям для разных случаев использования

### Общего назначения
- **llama3**: Лучшая универсальная модель
- **mistral**: Быстрая и эффективная альтернатива

### Генерация кода
- **codellama**: Специализирована для задач с кодом
- **deepseek-coder**: Альтернативная модель для кода

### Длинный контекст
- **llama3.1**: Поддерживает большие контекстные окна
- **mixtral**: Хороша для длинных документов

### Быстрые ответы
- **mistral**: Оптимизирована для скорости
- **phi**: Очень маленькая и быстрая
